{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "477f1a92-1ee3-4ec0-9a9c-8b1da39b05c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformer imported from local file \"transformer.py\"\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import string\n",
    "import random\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras import layers\n",
    "from keras.layers import Input, Embedding, Dense, Dropout, TextVectorization\n",
    "\n",
    "# Set working directory for transformer import and data loading\n",
    "os.chdir(f'{os.getenv(\"HOME\")}/Downloads/deeplearning')\n",
    "from transformer import Transformer\n",
    "\n",
    "os.chdir(f'{os.getenv(\"HOME\")}/Downloads/deeplearning')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4fae902c-8f52-46ee-87a4-38e63df7a32b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-01 19:21:17.840790: I metal_plugin/src/device/metal_device.cc:1154] Metal device set to: Apple M2\n",
      "2025-05-01 19:21:17.841020: I metal_plugin/src/device/metal_device.cc:296] systemMemory: 8.00 GB\n",
      "2025-05-01 19:21:17.841027: I metal_plugin/src/device/metal_device.cc:313] maxCacheSize: 2.67 GB\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1746141677.841573 31180663 pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "I0000 00:00:1746141677.841986 31180663 pluggable_device_factory.cc:271] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
     ]
    }
   ],
   "source": [
    "text_file = \"spa.txt\"\n",
    "with open(text_file) as f:\n",
    "    lines = f.read().split(\"\\n\")[:-1]\n",
    "\n",
    "text_pairs = []\n",
    "for line in lines:\n",
    "    english, spanish = line.split(\"\\t\")\n",
    "    spanish = \"[start] \" + spanish + \" [end]\"\n",
    "    text_pairs.append((english, spanish))\n",
    "\n",
    "random.shuffle(text_pairs)\n",
    "num_val = int(0.15 * len(text_pairs))\n",
    "num_train = len(text_pairs) - 2 * num_val\n",
    "train_pairs = text_pairs[:num_train]\n",
    "val_pairs = text_pairs[num_train:num_train+num_val]\n",
    "test_pairs = text_pairs[num_train+num_val:]\n",
    "\n",
    "vocab_size, seq_length = 15000, 20\n",
    "\n",
    "strip_chars = string.punctuation + \"¿\"\n",
    "strip_chars = strip_chars.replace(\"[\", \"\").replace(\"]\", \"\")\n",
    "\n",
    "def custom_standardization(input_string):\n",
    "    return tf.strings.regex_replace(tf.strings.lower(input_string), f\"[{re.escape(strip_chars)}]\", \"\")\n",
    "\n",
    "source_vectorization = TextVectorization(\n",
    "    max_tokens=vocab_size, output_mode=\"int\", output_sequence_length=seq_length)\n",
    "source_vectorization.adapt([pair[0] for pair in train_pairs])\n",
    "\n",
    "target_vectorization = TextVectorization(\n",
    "    max_tokens=vocab_size, output_mode=\"int\", standardize=custom_standardization,\n",
    "    output_sequence_length=seq_length + 1)\n",
    "target_vectorization.adapt([pair[1] for pair in train_pairs])\n",
    "\n",
    "def format_dataset(eng, spa):\n",
    "    eng = source_vectorization(eng)\n",
    "    spa = target_vectorization(spa)\n",
    "    return ((eng, spa[:, :-1]), spa[:, 1:])\n",
    "\n",
    "batch_size = 64\n",
    "def make_dataset(pairs):\n",
    "    eng_texts, spa_texts = zip(*pairs)\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((list(eng_texts), list(spa_texts)))\n",
    "    return dataset.batch(batch_size).map(format_dataset, num_parallel_calls=4).shuffle(2048).prefetch(16).cache()\n",
    "\n",
    "train_ds = make_dataset(train_pairs)\n",
    "val_ds = make_dataset(val_pairs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "026355fe-178a-4446-9f23-74c12fcce576",
   "metadata": {},
   "outputs": [],
   "source": [
    "def masked_loss(label, pred):\n",
    "    loss = keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')(label, pred)\n",
    "    mask = tf.cast(label != 0, dtype=loss.dtype)\n",
    "    loss *= mask\n",
    "    return tf.reduce_sum(loss) / tf.reduce_sum(mask)\n",
    "\n",
    "def masked_accuracy(label, pred):\n",
    "    pred = tf.argmax(pred, axis=2)\n",
    "    label = tf.cast(label, pred.dtype)\n",
    "    mask = label != 0\n",
    "    match = tf.cast((label == pred) & mask, dtype=tf.float32)\n",
    "    mask = tf.cast(mask, dtype=tf.float32)\n",
    "    return tf.reduce_sum(match) / tf.reduce_sum(mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7aaab1ad-eb21-40ad-bda5-8da13ab7d225",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomSchedule(keras.optimizers.schedules.LearningRateSchedule):\n",
    "    def __init__(self, d_model, warmup_steps=4000):\n",
    "        super().__init__()\n",
    "        self.d_model = tf.cast(d_model, tf.float32)\n",
    "        self.warmup_steps = warmup_steps\n",
    "\n",
    "    def __call__(self, step):\n",
    "        step = tf.cast(step, dtype=tf.float32)\n",
    "        arg1 = tf.math.rsqrt(step)\n",
    "        arg2 = step * (self.warmup_steps ** -1.5)\n",
    "        return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)\n",
    "\n",
    "learning_rate = CustomSchedule(128)\n",
    "optimizer = keras.optimizers.Adam(learning_rate, beta_1=0.9, beta_2=0.98, epsilon=1e-9)\n",
    "\n",
    "model = Transformer(n_layers=4, d_emb=128, n_heads=8, d_ff=512, dropout_rate=0.1,\n",
    "                    src_vocab_size=vocab_size, tgt_vocab_size=vocab_size)\n",
    "model.compile(loss=masked_loss, optimizer=optimizer, metrics=[masked_accuracy])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbd9f525-3ff0-4bf0-8629-e17dabfe10df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/heather/myvenv/tensorflow/lib/python3.12/site-packages/keras/src/layers/layer.py:939: UserWarning: Layer 'global_self_attention' (of type GlobalSelfAttention) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n",
      "/Users/heather/myvenv/tensorflow/lib/python3.12/site-packages/keras/src/layers/layer.py:939: UserWarning: Layer 'encoder_layer' (of type EncoderLayer) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n",
      "/Users/heather/myvenv/tensorflow/lib/python3.12/site-packages/keras/src/layers/layer.py:939: UserWarning: Layer 'encoder' (of type Encoder) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n",
      "/Users/heather/myvenv/tensorflow/lib/python3.12/site-packages/keras/src/layers/layer.py:939: UserWarning: Layer 'causal_self_attention' (of type CausalSelfAttention) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n",
      "/Users/heather/myvenv/tensorflow/lib/python3.12/site-packages/keras/src/layers/layer.py:939: UserWarning: Layer 'decoder_layer' (of type DecoderLayer) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n",
      "/Users/heather/myvenv/tensorflow/lib/python3.12/site-packages/keras/src/layers/layer.py:939: UserWarning: Layer 'decoder' (of type Decoder) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n",
      "2025-05-01 19:22:32.623399: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:117] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1302/1302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1165s\u001b[0m 878ms/step - loss: 7.5709 - masked_accuracy: 0.1552 - val_loss: 3.7973 - val_masked_accuracy: 0.4043\n",
      "Epoch 2/10\n",
      "\u001b[1m 191/1302\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m16:32\u001b[0m 894ms/step - loss: 3.8668 - masked_accuracy: 0.4072"
     ]
    }
   ],
   "source": [
    "model.fit(train_ds, epochs=10, validation_data=val_ds)\n",
    "model.save_weights(\"eng2spa_transformer_weights.keras\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "903cb066-3977-4991-a1a3-1cb9bc2d55ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "spa_vocab = target_vectorization.get_vocabulary()\n",
    "spa_index_lookup = dict(zip(range(len(spa_vocab)), spa_vocab))\n",
    "\n",
    "# Reload weights for inference\n",
    "model.load_weights(\"eng2spa_transformer_weights.keras\")\n",
    "\n",
    "def decode_sequence(input_sentence):\n",
    "    tokenized_input = source_vectorization([input_sentence])\n",
    "    decoded = \"[start]\"\n",
    "    for i in range(20):\n",
    "        tokenized_target = target_vectorization([decoded])[:, :-1]\n",
    "        preds = model.predict([tokenized_input, tokenized_target], verbose=0)\n",
    "        next_index = np.argmax(preds[0, i, :])\n",
    "        next_token = spa_index_lookup[next_index]\n",
    "        decoded += \" \" + next_token\n",
    "        if next_token == \"[end]\":\n",
    "            break\n",
    "    return decoded.replace(\"[start] \", \"\").replace(\" [end]\", \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0abd6f9f-5be6-49c0-bd05-3c87e7e3b4a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    print(\"\\nEnglish-to-Spanish Translator (Transformer-based)\")\n",
    "    print(\"Type 'quit' to exit.\\n\")\n",
    "    while True:\n",
    "        eng = input(\"Enter an English sentence: \")\n",
    "        if eng.strip().lower() == \"quit\":\n",
    "            break\n",
    "        print(\"Spanish Translation:\", decode_sequence(eng))\n",
    "        print(\"-\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (tensorflow)",
   "language": "python",
   "name": "myvenv-tf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
